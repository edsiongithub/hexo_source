---
layout: post
title:  python爬取历年杰青数据
date: 2021-04-29 09:42:27
tags:  
  - python
  - spyder
---


## 目标
1. 爬取网上公开的历年杰青数据
2. 清洗并存储在数据库中

## 工具
python、 beautifulsoup、 pandas、 sqlalchemy、mysql
<!--more-->

## 步骤

### 获取数据源
搜索引擎上按年份搜索关键字“杰青 名单”，在搜出的网页内，如提供表格形式的名单，则为爬取数据。
下面是我找到的2020年至2003（缺少2004年数据）的网页url。
``` python
url2003 = 'http://www.nsfc.gov.cn/nsfc/cen/temp/2003yy.html'
url2005 = 'http://www.gov.cn/zwgk/2005-08/22/content_25449.htm'
url2006 = 'http://www.edu.cn/xin_wen_gong_gao_1114/20060817/t20060817_192326.shtml'
url2007 = 'http://www.edu.cn/xin_wen_gong_gao_1114/20070813/t20070813_248811.shtml'
url2008 = 'http://www.cutech.edu.cn/cn/Fund/webinfo/2008/08/1238460679176533.htm'
url2009 = 'https://news.bioon.com/article/6351006.html'
url2010 ='http://news.sciencenet.cn/htmlnews/2010/8/235598.shtm'
url2011 = 'https://www.antpedia.com/news/29/n-158129.html'
url2012 = 'http://news.sciencenet.cn/htmlnews/2012/8/267906.shtm?id=267906'
url2013 = 'http://news.sciencenet.cn/htmlnews/2013/7/280577.shtm?id=280577'
url2014 = 'http://news.sciencenet.cn/htmlnews/2014/8/300367.shtm?id=300367'
url2015 = 'http://news.sciencenet.cn/htmlnews/2015/8/324190.shtm'
url2016 = 'http://news.sciencenet.cn/htmlnews/2016/8/352950.shtm'
url2017 = 'http://news.sciencenet.cn/htmlnews/2017/8/384206.shtm'
url2018 = 'http://news.sciencenet.cn/htmlnews/2018/8/416232.shtm' # 表头多一行
url2019 = 'http://news.sciencenet.cn/htmlnews/2019/8/428982.shtm' # 表头多一行
url2020 = 'http://news.sciencenet.cn/htmlnews/2020/8/443995.shtm'
```

### 编写爬虫程序
接下来直接上代码，python爬虫，根据不同url地址内的表格样式不同，代码需要微调一下。会在注释中增加说明
``` python
import requests
from bs4 import BeautifulSoup
import re
import sqlite3
import pandas as pd
import pymysql
from sqlalchemy import create_engine


conn = sqlite3.connect('jieqing.db')
url2003 = 'http://www.nsfc.gov.cn/nsfc/cen/temp/2003yy.html'
url2005 = 'http://www.gov.cn/zwgk/2005-08/22/content_25449.htm'
url2006 = 'http://www.edu.cn/xin_wen_gong_gao_1114/20060817/t20060817_192326.shtml'
url2007 = 'http://www.edu.cn/xin_wen_gong_gao_1114/20070813/t20070813_248811.shtml'
url2008 = 'http://www.cutech.edu.cn/cn/Fund/webinfo/2008/08/1238460679176533.htm'
url2009 = 'https://news.bioon.com/article/6351006.html'
url2010 ='http://news.sciencenet.cn/htmlnews/2010/8/235598.shtm'
url2011 = 'https://www.antpedia.com/news/29/n-158129.html'
url2012 = 'http://news.sciencenet.cn/htmlnews/2012/8/267906.shtm?id=267906'
url2013 = 'http://news.sciencenet.cn/htmlnews/2013/7/280577.shtm?id=280577'
url2014 = 'http://news.sciencenet.cn/htmlnews/2014/8/300367.shtm?id=300367'
url2015 = 'http://news.sciencenet.cn/htmlnews/2015/8/324190.shtm'
url2016 = 'http://news.sciencenet.cn/htmlnews/2016/8/352950.shtm'
url2017 = 'http://news.sciencenet.cn/htmlnews/2017/8/384206.shtm'
url2018 = 'http://news.sciencenet.cn/htmlnews/2018/8/416232.shtm' # 表头多一行
url2019 = 'http://news.sciencenet.cn/htmlnews/2019/8/428982.shtm' # 表头多一行
url2020 = 'http://news.sciencenet.cn/htmlnews/2020/8/443995.shtm'


url1 = 'https://www.sciping.com/22844.html'
url2 = 'https://www.sciping.com/23054.html'


def getHtmlText(url):
    """
    获取指定url的html源码
    """
    try:
        hd = {'user-agent': 'Chrome/89.0.4389.90'}
        r = requests.get(url, headers = hd)
        r.encoding = "utf-8"    #指定编码格式，防止中文乱码。浏览器console中输入document.charset,输出页面编码格式
        result = r.text
        return result
    except:
        return ''


def parseHtml(url):
    soup = BeautifulSoup(getHtmlText(url), 'html.parser')
    content = soup.select('table')[20] #这个table的索引可以在浏览器中查看元素，数一下数据表是整个html中table的顺序
    # 2005 年的处理一下多余字符
    content = re.sub(r'=\"\'.*', '', str(content))
    content = BeautifulSoup(content)
    # 2005 年的处理一下多余字符结束
    tbl = pd.read_html(content.prettify(),header = 0)[0] # 表头多一行的，header =1(2019、2018年数据)
    print(tbl) # 输出tbl,查看DataFrame结构
	return tbl



def write_to_sql(tbl, db = 'jieqing'):
    engin = create_engine('mysql+pymysql://root:mysqlpassword@mysqlhost:3306/jieqing')
    try:
        tbl.to_sql('jieqing', con = engin, if_exists = 'append', index = False)
    except Exception as e:
        print(e)


if __name__ == '__main__':
    tbl = parseHtml(url2005) #修改这里的url即可爬取不同年份的数据了
    write_to_sql(tbl) #写入mysql表中
```

### 数据清洗
由于历年的数据源表格形式不太一样，在写入mysql的时候，可能会出现一些问题。比如爬到的数据转换成pandas DataFrame后，
DataFrame表头跟数据库表字段匹配不上，这样写入数据就会出错。这时，临时修改一下mysql表列名，能跟DataFrame匹配上就可以了。
有些年份，并没有给出国别字段信息，这些都可以后期在mysql表中进行补充清洗。

## 总结
1. pandas 获取html中table结构数据方便，写入mysql更方便
2. 爬虫程序本身构造不复杂，遇到问题需要去慢慢摸索解决
3. 需要一些html前端技术，查看数据源网页上html源代码，来分析爬虫该如何获取数据。
